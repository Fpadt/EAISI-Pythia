---
title   : EAISI - Pythia
subtitle: Data Ingestion
author  : "F.J. Padt"
date    : "`r format(Sys.time(), '%B %d, %Y')`"
output  :
  pdf_document:
    df_print: paged
    toc: yes
    toc_depth: 1      
editor_options:
  chunk_output_type: console
  markdown: 
    wrap: 80  
---

\newpage

![Logo](C:/PW\OneDrive\ET\pythia\img\pythia_logo2_no_text.png)

# Purpose

Refresh data sets for the Pythia project from BW OpenHub to Pythia on PET. The
data sets are used to validate the Pythia model.

This code needs to run in Ecotone Network and SAP should be available

# Setup

```{r}
#| label:  setup
#| eval:   true

# knitr::opts_chunk$set(
#   cache   = FALSE,
#   echo    = TRUE,     # include R source code in the output  
#   # eval    = TRUE,
#   message = FALSE,
#   warning = FALSE,
#   results = "markup",
#   image   = TRUE,  
#   include = TRUE      # include the chunk output in the output 
# )

SID <- "WPB500"
SYS <- substr(SID, 1, 3)

# SAP Access
# library(reticulate)
# use_condaenv("sapyr")

# public functions ---------------------------------------------------------
invisible(source('library/KnitR_SetUp.R'))

lsrc <- "notebooks/05_data_ingestion.R"  
if( file.exists(lsrc)){source(lsrc)}

```

```{r}
#| label: 'get path'
#| eval:   false
#| 
clipr::write_clip(
  normalizePath(
    file.path(PS01, SYS, "RTP", "OB")))
```

# Export & Copy Procedure

## Pythia DTP's

| Seq. | DTP | Description | Time |
|-------------------|-------------------|-------------------|-----------------------|
| 0 | DTP_006EIZGR39XAWQANS1LNUZF2Y | Pythia's Advice -\> IS PERKZ-W \[2021\] Incl.FR50 | 15 min. |
| 0 | DTP_006EIZGR39XAWQANYOZZI4HSQ | Pythia's Advice -\> IS PERKZ-W \[2022\] Incl.FR50 |  |
| 0 | DTP_006EIZGR39XAWQANZ3O0NUFIY | Pythia's Advice -\> IS PERKZ-W \[2023\] Incl.FR50 |  |
| 1 | DTP_006EIZGR39XAWQANZFHWJ64UY |  |  |
| 2 | DTP_006EIZGR39XAWPP4O3JPT9E6Y | PYTHIA -\> IS PERKZ-W \[\>=2024\] | 05 min |
| 3 | DTP_006EIZGR39XAWPP4SWFV127TM | OS PERKZ-W \[\>=SY-DATE - 60\] | 02 min |

## Export

1.  \[15. Min\] RSPC: LC_DYN_PERKZ_W
2.  \[05. Min\] DTP_006EIZGR39XAWPP4O3JPT9E6Y PYTHIA -\> IS PERKZ-W \[\>=2024\]
3.  AL11 - DSCP E:\USR\SAP\STAGE\DSCP\PERKZ\_W\
4.  save PRTP
    1.  TMP execute next code chunk to change Thousands Separator
5.  

## Transaction Data

```{r}
#| label: 'get Header DD_SALES_QTY'
#| eval:   false
 
# HDR <-  
#   fread( 
#     file = file.path(PS01, SYS, "RTP", "CSV", paste0("S_DD_SALES_QTY.CSV")), 
#     header = TRUE, skip=5 
#   ) 
```

### Outbound

#### Functions

```{r}
#| label: 'Outbound - Data Ingestion Functions'
#| eval:   true

dtDSCP_TRAN <- 
  fread(text = "
        FIELDNM   , FIELDTP, FIELDLEN, DECIMALS, KEYFLAG, TEXT          , DATATYPE
        MATERIAL  , VARCHAR, 18      ,        0,       X, Material      , CHAR
        CUSTOMER  , VARCHAR, 10      ,        0,       X, Customer      , CHAR
        PLANT     , VARCHAR,  4      ,        0,       X, Plant         , CHAR
        SALESORG  , VARCHAR,  4      ,        0,       X, SalesOrg      , CHAR
        CALDAY    , DATE   ,  8      ,        0,       X, CalDay        , DATS
        SLS_QT_SO , FLOAT  ,  8      ,        0,       X, Sales         , QUAN
        SLS_QT_RET, FLOAT  ,  8      ,        0,       X, Returns       , QUAN
        SLS_QT_FOC, FLOAT  ,  8      ,        0,       X, Free-of-Charge, QUAN
        SLS_QT_DIR, FLOAT  ,  8      ,        0,       X, Direct        , QUAN
        SLS_QT_PRO, FLOAT  ,  8      ,        0,       X, Promotion     , QUAN
        SLS_QT_IC , FLOAT  ,  8      ,        0,       X, Intercompany  , QUAN
        MSQTBUO   , FLOAT  ,  8      ,        0,       X, MSQTBUO       , QUAN"
  ) 

generate_input_columns_definition <- 
  function(x) {
    # Calculate the maximum length of FIELDNM for alignment
    x[, no_spc := max(nchar(FIELDNM)) - nchar(FIELDNM) + 3]
    
    # Generate the formatted string using glue_data
    result <- x[, glue_data(.SD, 
                            "'{FIELDNM}'{strrep(' ', no_spc)}: '{FIELDTP}',"
    )]
    
    # Collapse the vector into a single string
    result <- paste0(result, collapse = "\n")
    
    return(result)
  }


# COLUMN_DEF   <-  list(
#   MATERIAL   = 'VARCHAR',
#   CUSTOMER   = 'VARCHAR',
#   PLANT      = 'VARCHAR',
#   SALESORG   = 'VARCHAR',
#   CALDAY     = 'DATE',
#   SLS_QT_SO  = 'FLOAT',
#   SLS_QT_RET = 'FLOAT',
#   SLS_QT_FOC = 'FLOAT',
#   SLS_QT_DIR = 'FLOAT',
#   SLS_QT_PRO = 'FLOAT',
#   SLS_QT_IC  = 'FLOAT',
#   MSQTBUO    = 'FLOAT'
# )

# Configuration parameters of Open Hub DSCP_TRAN
DSCP_TRAN <- list(
  DELIM        = ';',
  HEADER       = FALSE,
  DATE_FORMAT  = '%Y-%m-%d'
  # ,
  # COLUMN_DEF   = list(
  #   MATERIAL   = 'VARCHAR',
  #   CUSTOMER   = 'VARCHAR',
  #   PLANT      = 'VARCHAR',
  #   SALESORG   = 'VARCHAR',
  #   CALDAY     = 'DATE',
  #   SLS_QT_SO  = 'FLOAT',
  #   SLS_QT_RET = 'FLOAT',
  #   SLS_QT_FOC = 'FLOAT',
  #   SLS_QT_DIR = 'FLOAT',
  #   SLS_QT_PRO = 'FLOAT',
  #   SLS_QT_IC  = 'FLOAT',
  #   MSQTBUO    = 'FLOAT'
  # )
)

transformation_columns <- c(
  "lpad(MATERIAL, 18, '0') AS MATERIAL",
  "lpad(CUSTOMER, 10, '0') AS CUSTOMER",
  "PLANT AS PLANT",
  "SALESORG AS SALESORG",
  "CALDAY AS CALDAY",
  "DATE_TRUNC('month', CALDAY) AS CALMONTH",
  "SLS_QT_SO AS SLS_QT_SO",
  "SLS_QT_RET AS SLS_QT_RET",
  "SLS_QT_FOC AS SLS_QT_FOC",
  "SLS_QT_DIR AS SLS_QT_DIR",
  "SLS_QT_PRO AS SLS_QT_PRO",
  "SLS_QT_IC AS SLS_QT_IC",
  "MSQTBUO AS MSQTBUO"
)

# Function to generate formatted columns
# generate_formatted_columns <-
#   function(column_definitions) {
#     # pad space to get same length
#     nms <- names(column_definitions)
#     mxl <- nms %>% nchar() %>% max()
# 
#     paste(
#       sapply(
#         names(column_definitions) ,
#         function(col) {
#           lng <- mxl + 2 - nchar(col)
#           cln <- str_pad(": ", lng, side = "left", pad = " ")
#           glue("'{col}'{cln}'{column_definitions[[col]]}'")
#         }
#       ),
#       collapse = ",\n      "
#     )
#   }

# Function to generate the read_csv SQL snippet
generate_duckdb_read_csv <- 
  function(ffns, delim, header, date_format, formatted_columns) {
    glue("
    read_csv('{ffns}',
      delim      = '{delim}',
      header     = {header},
      dateformat = '{date_format}',
      columns = {{
        {formatted_columns}
      }}
    )
  ")
  }

# Function to perform data transformation
# transform_data_sql3 <-
#   function(duckdb_read_csv_sql, transformation,con) {
# 
#     # Use glue_sql to construct SQL query
#     sql_get_data <-
#       glue_sql("
#         SELECT
#          *
#         FROM {DBI::SQL(duckdb_read_csv_sql)}
#         ", .con = con)
# 
#     sql_transform_data <-
#       glue_sql("
#         SELECT
#           lpad(MATERIAL, 18, '0')     AS MATERIAL,
#           lpad(CUSTOMER, 10, '0')     AS CUSTOMER,
#           PLANT                       AS PLANT,
#           SALESORG                    AS SALESORG,
#           CALDAY                      AS CALDAY,
#           DATE_TRUNC('month', CALDAY) AS CALMONTH,
#           SLS_QT_SO                   AS SLS_QT_SO,
#           SLS_QT_RET                  AS SLS_QT_RET,
#           SLS_QT_FOC                  AS SLS_QT_FOC,
#           SLS_QT_DIR                  AS SLS_QT_DIR,
#           SLS_QT_PRO                  AS SLS_QT_PRO,
#           SLS_QT_IC                   AS SLS_QT_IC,
#           MSQTBUO                     AS MSQTBUO
#         FROM ({DBI::SQL(sql_get_data)})
#         ", .con = con)
# 
#     return(sql_transform_data)
#   }

transform_data_sql <- function(duckdb_read_csv_sql, transformation, con) {
  # Use glue_sql to construct SQL query
  sql_get_data <- 
    glue_sql("
      SELECT 
       *
      FROM {DBI::SQL(duckdb_read_csv_sql)}
      ", .con = con)
  
  # Create the SELECT statement dynamically
  select_statement <- paste(transformation, collapse = ",\n          ")
  
  sql_transform_data <- 
    glue_sql("
      SELECT 
        {DBI::SQL(select_statement)}
      FROM ({DBI::SQL(sql_get_data)})
      ", .con = con)
  
  return(sql_transform_data)
}


# Function to write data to Parquet
write_data_to_parquet <- function(sql_transform_data, output_file) {
  sql_write_data <- glue("
    COPY ({sql_transform_data})
    TO '{output_file}'
    (FORMAT 'parquet', CODEC 'uncompressed')
    ")
  
  return(sql_write_data)
}

# Main function to process data
process_data <- 
  function(
    yr, 
    input_base_path, 
    output_base_path, 
    sys_value, 
    verbose = FALSE) {
    
    # Define file paths
    input_csv_file  <- 
      file.path(
        input_base_path , sys_value, "RTP", "OB",  
        paste0("DD_SALES_QTY_20", yr, ".CSV")
      )
    output_pqt_file <- 
      file.path(
        output_base_path, sys_value, "RTP", "OB",
        paste0("DD_SALES_QTY_20", yr, ".parquet")
      )
    
    # Check if input file exists
    if (!file.exists(input_csv_file)) {
      stop(glue("Input CSV file does not exist: {input_csv_file}"))
    }
    
    # Generate formatted columns
    formatted_columns <- generate_input_columns_definition(dtDSCP_TRAN)
    #generate_formatted_columns(DSCP_TRAN$COLUMN_DEF)
    
    # Generate SLS_CSV
    duckdb_read_csv <- 
      generate_duckdb_read_csv(
        ffns              = input_csv_file,
        delim             = DSCP_TRAN$DELIM,
        header            = DSCP_TRAN$HEADER,
        date_format       = DSCP_TRAN$DATE_FORMAT,
        formatted_columns = formatted_columns
      )
    
    # Establish a connection to DuckDB
    con <- dbConnect(duckdb(), dbdir = ":memory:")
    
    # Ensure the connection is closed when the function exits
    on.exit(dbDisconnect(con), add = TRUE)
    
    # Generate SQL for data transformation
    sql_transform_data <- 
      transform_data_sql(
        duckdb_read_csv_sql = duckdb_read_csv,
        transformation      = transformation_columns,
        con                 = con
      )
    
    # Generate SQL for writing data
    sql_write_data <- write_data_to_parquet(sql_transform_data, output_pqt_file)
    
    # Execute the SQL statements
    tryCatch({
      system.time({
        dbExecute(con, sql_write_data)
        if (verbose == TRUE){
          print(
            dbGetQuery(con, glue("DESCRIBE SELECT * FROM '{output_pqt_file}';"))
          )
        }
        message(
          cat(white$bgGreen$bold(glue("Data successfully written to {output_pqt_file}")))
        )
      })
    }, error = function(e) {
      message(
        cat(white$bgRed$bold(glue("An error occurred: {e$message}")))
      )
    })
  }

```

#### Execution

```{r}
#| label: 'Data Ingestion Execution from Bronze (S1B) to Silver (S2S)'
#| eval:   true

yrs <- 24:24  # Year parameter

# Run the main function
purrr::walk(yrs, process_data, PS01, PS02, SYS, verbose = TRUE)

```

### Inbound

#### Functions

```{r}
#| label: 'Inbound - Data Ingestion Functions'
#| eval:   true

# Configuration parameters of DataSources:
# - BEFOREDR_ACTUALS
# - BEFOREDR_FORECASTS
# - AFTERDR_ACTUALS
# - AFTERDR_FORECASTS

# Configuration parameters of Open Hub DSCP_TRAN
DSCP_INBOUND <- list(
  DELIM        = ';',
  HEADER       = FALSE,
  DATE_FORMAT  = '%Y-%m-%d',
  COLUMN_DEFINITIONS = list(
    CREATION_DATE          = 'DATE',
    DFU_CODE               = 'VARCHAR',
    MATERIAL_CODE          = 'VARCHAR',
    CUSTOMERL3_CODE        = 'VARCHAR',
    CL3_CODE               = 'VARCHAR',
    CL2_CODE               = 'VARCHAR',
    CL1_CODE               = 'VARCHAR',
    PLANT_CODE             = 'VARCHAR',
    SALESORG_CODE          = 'VARCHAR',
    PERIOD_CODE            = 'VARCHAR',
    TOTAL_DEMAND_QTY       = 'FLOAT',
    BASE_LINE_QTY          = 'FLOAT',
    PROMO_QTY              = 'FLOAT',
    TOTAL_DEMAND_COMP_QTY  = 'FLOAT',
    PROMO_COMP_QTY         = 'FLOAT',
    BASEUNITOFMEASURE_CODE = 'VARCHAR'
  )
)

transformation_columns <- 
  "CREATION_DATE          AS CREATION_DATE,
   DFU_CODE               AS DFU_CODE,
   MATERIAL_CODE          AS MATERIAL_CODE,
   CUSTOMERL3_CODE        AS CUSTOMERL3_CODE,
   CL3_CODE               AS CL3_CODE,
   CL2_CODE               AS CL2_CODE,
   CL1_CODE               AS CL1_CODE,
   PLANT_CODE             AS PLANT_CODE,
   SALESORG_CODE          AS SALESORG_CODE,
   PERIOD_CODE            AS PERIOD_CODE,
   TOTAL_DEMAND_QTY       AS TOTAL_DEMAND_QTY,
   BASE_LINE_QTY          AS BASE_LINE_QTY,
   PROMO_QTY              AS PROMO_QTY,
   TOTAL_DEMAND_COMP_QTY  AS TOTAL_DEMAND_COMP_QTY,
   PROMO_COMP_QTY         AS PROMO_COMP_QTY,
   BASEUNITOFMEASURE_CODE AS BASEUNITOFMEASURE_CODE"

# # Function to generate formatted columns
generate_formatted_columns <-
  function(column_definitions) {
    paste(
      sapply(
        names(column_definitions),
        function(col) {
          glue("'{col}'  : '{column_definitions[[col]]}'")
        }
      ),
      collapse = ",\n    "
    )
  }

# # Function to generate the read_csv SQL snippet
# generate_sls_csv <- function(ffns, delim, header, date_format, formatted_columns) {
#   glue("
#     read_csv('{ffns}',
#       delim      = '{delim}',
#       header     = {header},
#       dateformat = '{date_format}',
#       columns = {{
#         {formatted_columns}
#       }}
#     )
#   ")
# }

# # Function to perform data transformation
transform_data_sql2 <- function(sls_csv_sql, con) {
  # Use glue_sql to construct SQL query
  sql_get_data <- glue_sql("
    SELECT *
    FROM {DBI::SQL(sls_csv_sql)}
    ", .con = con)

  sql_transform_data <- glue_sql("
    SELECT
      lpad(MATERIAL, 18, '0')     AS MATERIAL,
      lpad(CUSTOMER, 10, '0')     AS CUSTOMER,
      PLANT                       AS PLANT,
      SALESORG                    AS SALESORG,
      CALDAY                      AS CALDAY,
      DATE_TRUNC('month', CALDAY) AS CALMONTH,
      SLS_QT_SO                   AS SLS_QT_SO,
      SLS_QT_RET                  AS SLS_QT_RET,
      SLS_QT_FOC                  AS SLS_QT_FOC,
      SLS_QT_DIR                  AS SLS_QT_DIR,
      SLS_QT_PRO                  AS SLS_QT_PRO,
      SLS_QT_IC                   AS SLS_QT_IC,
      MSQTBUO                     AS MSQTBUO
    FROM ({DBI::SQL(sql_get_data)})
    ", .con = con)

  return(sql_transform_data)
}
# 
# # Function to write data to Parquet
# write_data_to_parquet <- function(sql_transform_data, output_file) {
#   sql_write_data <- glue("
#     COPY ({sql_transform_data})
#     TO '{output_file}'
#     (FORMAT 'parquet', CODEC 'uncompressed')
#     ")
#   
#   return(sql_write_data)
# }

# Main function to process data
process_data2 <- 
  function(
    input_base_path, 
    output_base_path, 
    sys_value = "WTB", 
    verbose = FALSE) {
    
    # Define file paths
    input_csv_file  <- 
      file.path(
        input_base_path , sys_value, "RTP", "IB",
        paste0("OUT_BI_DATA_PREDR_ACTUALS.CSV")
      )
    output_pqt_file <- 
      file.path(
        output_base_path, sys_value, "RTP", "IB",
        paste0("OUT_BI_DATA_PREDR_ACTUALS.parquet")
      )
    
    # Check if input file exists
    if (!file.exists(input_csv_file)) {
      stop(glue("Input CSV file does not exist: {input_csv_file}"))
    }
    
    # Generate formatted columns
    formatted_columns <- 
      generate_formatted_columns(DSCP_INBOUND$COLUMN_DEF)
    
    # Generate SLS_CSV
    duckdb_read_csv <- 
      generate_duckdb_read_csv(
        ffns              = input_csv_file,
        delim             = DSCP_INBOUND$DELIM,
        header            = DSCP_INBOUND$HEADER,
        date_format       = DSCP_INBOUND$DATE_FORMAT,
        formatted_columns = formatted_columns
      )
    
    # Establish a connection to DuckDB
    con <- dbConnect(duckdb(), dbdir = ":memory:")
    
    # Ensure the connection is closed when the function exits
    on.exit(dbDisconnect(con), add = TRUE)
    
    # Generate SQL for data transformation
    sql_transform_data <- 
      transform_data_sql2(
        duckdb_read_csv_sql = duckdb_read_csv,
        transformation      = transformation_columns,
        con                 = con
      )
    
    # Generate SQL for writing data
    sql_write_data <- write_data_to_parquet(sql_transform_data, output_pqt_file)
    
    # Execute the SQL statements
    tryCatch({
      system.time({
        dbExecute(con, sql_write_data)
        if (verbose == TRUE){
          print(
            dbGetQuery(con, glue("DESCRIBE SELECT * FROM '{output_pqt_file}';"))
          )
        }
          message(
            cat(white$bgGreen$bold(glue("Data successfully written to {output_pqt_file}")))
            )
      })
    }, error = function(e) {
        message(
          cat(white$bgRed$bold(glue("An error occurred: {e$message}")))
          )
    })
  }

```

#### Execution

```{r}
#| label: 'Data Ingestion Execution from Bronze (S1B) to Silver (S2S)'
#| eval:   true

# yrs <- 24:24  # Year parameter

# Run the main function
# purrr::walk(yrs, process_data, PS01, PS02, SYS, verbose = TRUE)
process_data2(PS01, PS02, "WTB", verbose = TRUE)

```

```{r}
#| label: 'functions'
#| eval:  true

getHeaderDataSource <-
  function(x){
    fread(file = file.path(PS01, SYS, "B4", "B4_RSDSSEGFD.csv"))     %T>%
    setorder(DATASOURCE, POSIT)                                       %>%
    .[DATASOURCE == x, FIELDNM]
}

```

```{r}
#| label: 'Data Ingestion Execution'
#| eval:   true

files_Inbound <- 
  list.files(file.path(PS01, SYS, "RTP", "IB"), pattern = "*.CSV", full.names = TRUE)

PRE_A <-  fread(
  cmd = paste0("iconv -f UTF-16 -t UTF-8 '", files_Inbound[1], "'")
) %T>%
setnames(getHeaderDataSource("PRE_A")) 


empty_unit <- PRE_A[BASEUNITOFMEASURE_CODE == ""]
neg_baseln <- PRE_A[BASE_LINE_QTY < 0]


```

## Master Data

### Material

#### MATERIAL

```{r}
#| label: 'MATERIAL'
#| eval:   true

pAREA <- "MATERIAL"
MATERIAL <- 
  fLoadOpenHubExport(
    pAREA = pAREA,  
    pKEY  = c(pAREA),
    pPTH  = file.path(PS01, SYS, "RTP", "CSV")
  )                                                                       %>% 
 .[, MATERIAL:= LP0(MATERIAL, 18)]                                        %T>%
  setcolorder("MATERIAL")                                                 %T>%
  write_parquet(
    sink = file.path(
      PS02, SYS, "RTP",  
      paste0(CFG[EXP == "NEW" & AREA == pAREA, BNM], ".parquet")
    )
  )

```

#### MAT_SALES

```{r}
#| label: 'MAT_SALES',
#| eval:   true

pAREA <- "MAT_SALES"
MAT_SALES <- 
  fLoadOpenHubExport(
    pAREA = pAREA,  
    pPTH  = file.path(PS01, SYS, "RTP", "CSV")
  )                                                                       %>%
  .[, DISTR_CHAN:= 10]                                                    %>%
  .[, `:=` (MAT_SALES = LP0(MATERIAL, 18), MATERIAL = NULL)]              %T>%
  setcolorder(c("MAT_SALES", "SALESORG", "DISTR_CHAN"))                   %T>%       
  setkey("MAT_SALES", "SALESORG", "DISTR_CHAN")                           %T>%
  write_parquet(
    sink = file.path(
      PS02, SYS, "RTP", 
      paste0(CFG[EXP == "NEW" & AREA == pAREA, BNM], ".parquet")
    )
  )                                                              

```

#### MAT_PLANT

```{r}
#| label: 'MAT_PLANT',
#| eval:   true

pAREA <- "MAT_PLANT"
MAT_PLANT <- 
  fLoadOpenHubExport(
    pAREA = pAREA,  
    pKEY  = c("MAT_PLANT", "PLANT"),
    pPTH  = file.path(PS01, SYS, "RTP", "CSV")
  )                                                                       %>%
  .[, `:=` (MAT_PLANT = LP0(MAT_PLANT, 18))]                              %T>%
  setcolorder(c("MAT_PLANT", "PLANT"))                                    %T>%       
  setkey("MAT_PLANT", "PLANT")                                            %T>%
  write_parquet(
    sink = file.path(
      PS02, SYS, "RTP", 
      paste0(CFG[EXP == "NEW" & AREA == pAREA, BNM], ".parquet")
    )
  ) 
```

### Customer

#### CUST_SALES

```{r}
#| label: 'CUST_SALES',
#| eval:   true

pAREA <- "SOLDTO"
CUST_SALES <- 
  fLoadOpenHubExport(
    pAREA = pAREA,  
    # pKEY  = c("MAT_PLANT", "PLANT"),
    pPTH  = file.path(PS01, SYS, "RTP", "CSV")
  )                                                                       %>%
  .[, `:=` (
    CUST_SALES = LP0(CUSTOMER  , 10), 
    CUSTHIE04  = LP0(CUSTHIE04 , 10), 
    CUST_HIE03 = LP0(CUST_HIE03, 10), 
    CUST_HIE02 = LP0(CUST_HIE02, 10), 
    CUST_HIE01 = LP0(CUST_HIE01, 10),     
    DISTR_CHAN = 10, 
    CUSTOMER   = NULL
    )]                                                                    %T>%
  setcolorder(c("CUST_SALES", "SALESORG", "DISTR_CHAN"))                  %T>%         
  setkey("CUST_SALES", "SALESORG", "DISTR_CHAN")                          %T>%
  write_parquet(
    sink = file.path(
      PS02, SYS, "RTP",  
      paste0(CFG[EXP == "NEW" & AREA == pAREA, BNM], ".parquet")
    )
  ) 
```

### Stock

```{r}
#| label: 'Stock'
#| eval:   true

pAREA <- "STOCK"
STK <- 
  fLoadOpenHubExport(
    pAREA = pAREA,  
    # pKEY  = c("MAT_PLANT", "PLANT"),
    pPTH  = file.path(PS01, SYS, "STK", "CSV")
  )                                                              %T>%         
  setkey("CALDAY", "MATERIAL", "PLANT")                          %T>%
  write_parquet(
    sink = file.path(
      PS02, SYS, "STK", 
      paste0(CFG[EXP == "NEW" & AREA == pAREA, BNM], ".parquet")
    )
  ) 

```

### Promotions

#### PromoNat

```{r}
#| label: 'PROMO'
#| eval:   true

REL_FLDS <- 
  wb_to_df(
    file  = file.path(PS01, "PRM", "PROMONAT.xlsx"),
    sheet = "FIELDS",
    cols  = c(2, 4, 5)
  )                         %>%
  setDT()                   %>%
  .[RELEVANT == "YES", COL] %>%
  sort()

PROMONAT <-
  wb_to_df(
    file  = file.path(PS01, "PRM", "PROMONAT.xlsx"),
    sheet = "PROMONAT",
    cols  = REL_FLDS
  )                                                           %T>%
  write_parquet(
    sink = file.path(
      PS02, "PRM", 
      "PROMONAT.parquet"
    )
  )                                                     

CR_PR_PROMO_CL2_HEADER <- 
  fread(
    file = file.path(PPRM, "CR_PR_PROMO_CL2_HEADER.CSV")
  )

CR_PR_PROMO_CL2_LINE <- 
  fread(
    file = file.path(PPRM, "CR_PR_PROMO_CL2_LINE.CSV")
  )

CR_PR_PROMO_CL3_HEADER <- 
  fread(
    file = file.path(PPRM, "CR_PR_PROMO_CL3_HEADER.CSV")
  )

CR_PR_PROMO_CL3_LINE <- 
  fread(
    file = file.path(PPRM, "CR_PR_PROMO_CL3_LINE.CSV")
  )
```
